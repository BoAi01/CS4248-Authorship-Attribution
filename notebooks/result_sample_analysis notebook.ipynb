{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a model checkpoint and run it on the test split of a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f022c084c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "RS = 123\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_list.pt stores the stacked predictions of the test samples in the format of [#samples, #classes]; label_list stores the true labels of all samples in a list of length #samples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aibo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aibo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas.core import base\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "from dataset import NumpyDataset, TransformerEnsembleDataset, TrainSamplerMultiClass, TrainSampler, TrainSamplerMultiClassUnit\n",
    "from models import AggregateFeatEnsemble, DynamicWeightEnsemble, LogisticRegression, BertClassiferHyperparams, SimpleEnsemble, FixedWeightEnsemble\n",
    "from utils import *  # bad practice, nvm\n",
    "from contrastive_utils import compute_sim_matrix, compute_target_matrix, contrastive_loss\n",
    "\n",
    "ckpt_dir = '../exp_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_dataframe(source):\n",
    "    print(\"Loading and processing dataframe\")\n",
    "\n",
    "    dataset_path = \"datasets\"\n",
    "\n",
    "    dataset_file_name = {\n",
    "        \"enron\": 'full_enron.csv',\n",
    "        \"imdb\": 'full_imdb_feat.csv',\n",
    "        \"imdb62\": 'full_imdb62.csv',\n",
    "        \"blog\": 'full_blog.csv',\n",
    "        \"ccat50\": \"ccat50-auth-index.csv\"\n",
    "    }\n",
    "\n",
    "    df = None\n",
    "\n",
    "    if source == \"enron\":\n",
    "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
    "        df['name'] = df['From'].apply(lambda x: x.split(\"'\")[1].split(\".\")[0])\n",
    "        df['name_in_mail'] = df.apply(lambda x: is_name_in_email(x['name'], x['content']), axis=1)\n",
    "        df = df[df['name_in_mail'] == 0]\n",
    "        df = df[df['content'].apply(lambda x: '-----' not in str(x))]\n",
    "        df = df[df['content'].apply(lambda x: \"@\" not in str(x))]\n",
    "        df = df[df['content'].apply(lambda x: \"From: \" not in str(x))]\n",
    "        df = df[df['content'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "        df.to_csv(os.path.join(dataset_path, 'full_enron2.csv'))\n",
    "\n",
    "    elif source == \"imdb\":\n",
    "        feat_path = os.path.join(dataset_dir, \"full_imdb_feat.csv\")\n",
    "        if os.path.isfile(feat_path):\n",
    "            df = pd.read_csv(feat_path, index_col=0)\n",
    "        else:\n",
    "            # # Parallelize apply on Pandas\n",
    "            from pandarallel import pandarallel\n",
    "            pandarallel.initialize()\n",
    "\n",
    "            df = pd.read_csv(os.path.join(dataset_dir, 'full_imdb.csv'), index_col=0)\n",
    "            print(\"drop rows!!!!!!!!!!!!!\")\n",
    "            drop_count = 0\n",
    "            for index, row in df.iterrows():\n",
    "                # print(row['content'])\n",
    "                if len(str(row['content'])) <= 3:\n",
    "                    df.drop(index, inplace=True)\n",
    "                    drop_count += 1\n",
    "            print(f\"dropped {drop_count} rows\")\n",
    "            print(df.shape)\n",
    "            df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
    "            df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\",\n",
    "                \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\",\n",
    "                \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\",\n",
    "                \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\",\n",
    "                \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
    "            df.to_csv(feat_path)\n",
    "\n",
    "    elif source == \"imdb62\":\n",
    "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, \"full_imdb62.csv\"), index_col=0)\n",
    "\n",
    "    elif source == \"blog\":\n",
    "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
    "\n",
    "    elif source == \"ccat50\":\n",
    "        feat_path = os.path.join(dataset_dir, \"full_ccat50_feat.csv\")\n",
    "        if os.path.isfile(feat_path):\n",
    "            df = pd.read_csv(feat_path, index_col=0)\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
    "            from pandarallel import pandarallel\n",
    "            pandarallel.initialize()\n",
    "            df['content_tfidf'] = df['content'].parallel_apply(lambda x: process(x))\n",
    "            df[[\"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\", \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\",\n",
    "                    \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\", \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\",\n",
    "                    \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\", \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\",\n",
    "                    \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\", \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\",\n",
    "                    \"f_e_10\", \"f_e_11\", \"richness\"]] = df['content'].parallel_apply(lambda x: extract_style(x))\n",
    "        df.to_csv(feat_path)\n",
    "    elif source == \"turing\":\n",
    "        df = pd.read_csv(os.path.join(dataset_path, \"turing_AA_train_test.csv\"))\n",
    "        df.sort_values(by=['train', 'From'], inplace=True, ascending=[False, True])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bert(nlp_train, nlp_test, tqdm_on, num_authors = 0, \n",
    "              return_features=True, model_name='microsoft/deberta-base', embed_len=768, ckpt_name='', table_name=''):\n",
    "\n",
    "    print(\"#####\")\n",
    "    print(\"Training BERT\")\n",
    "    from models import LogisticRegression\n",
    "    from dataset import BertDataset\n",
    "    from models import BertClassifier\n",
    "\n",
    "    tokenizer, extractor = None, None\n",
    "    if 'bert-base' in model_name:\n",
    "        from transformers import BertTokenizer, BertModel\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        extractor = BertModel.from_pretrained(model_name)\n",
    "    elif 'deberta' in model_name:\n",
    "        from transformers import DebertaTokenizer, DebertaModel\n",
    "        tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
    "        extractor = DebertaModel.from_pretrained(model_name)\n",
    "    elif 'roberta' in model_name:  # roberta-base\n",
    "        from transformers import RobertaTokenizer, RobertaModel\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        extractor = RobertaModel.from_pretrained(model_name)\n",
    "    elif 'gpt2' in model_name:\n",
    "        from transformers import GPT2Tokenizer, GPT2Model\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        extractor = GPT2Model.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # for gpt tokenizer only\n",
    "    elif 'gpt' in model_name:  # 'openai-gpt'\n",
    "        from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name)\n",
    "        extractor = OpenAIGPTModel.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.unk_token  # for gpt tokenizer only\n",
    "        print(f'pad token {tokenizer.unk_token}')\n",
    "    elif 'xlnet' in model_name:\n",
    "        from transformers import XLNetTokenizer, XLNetModel\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "        extractor = XLNetModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"model {model_name} not implemented\")\n",
    "\n",
    "    # freeze extractor\n",
    "    for param in extractor.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # business logic\n",
    "    #train_x, train_y = nlp_train['content'].tolist(), nlp_train['Target'].tolist()\n",
    "    #val_x, val_y = nlp_val['content'].tolist(), nlp_val['Target'].tolist()\n",
    "    test_x, test_y = nlp_test['content'].tolist(), nlp_test['Target'].tolist()\n",
    "\n",
    "    # training setup\n",
    "    num_epochs, base_lr, base_bs, ngpus, dropout = 1, 1e-5, 8, torch.cuda.device_count(), 0.35\n",
    "    num_tokens, hidden_dim, out_dim = 256, 512, max(test_y) + 1\n",
    "    model = BertClassifier(extractor, LogisticRegression(embed_len * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
    "#     model.load_state_dict(torch.load(\"./exp_data/3b_val_bert-base-cased_coe1.0_temp0.1_unit2_epoch8/3b_val_val0.61900_finale7.pt\"))\n",
    "    model.load_state_dict(torch.load(f'./exp_data/{ckpt_name}'))\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=base_lr * ngpus, weight_decay=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)\n",
    "\n",
    "#     train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
    "    #val_set = BertDataset(val_x, val_y, tokenizer, num_tokens)\n",
    "    test_set = BertDataset(test_x, test_y, tokenizer, num_tokens, return_idx=True)\n",
    "\n",
    "    coefficient, temperature, sample_unit_size = 0.0, 0.1, 2\n",
    "    print(f'coefficient, temperature, sample_unit_size = {coefficient, temperature, sample_unit_size}')\n",
    "\n",
    "    # recorder\n",
    "    exp_dir = os.path.join(ckpt_dir, f'{id}_{model_name.split(\"/\")[-1]}_coe{coefficient}_temp{temperature}_unit{sample_unit_size}_epoch{num_epochs}')\n",
    "    writer = SummaryWriter(os.path.join(exp_dir, 'board'))\n",
    "\n",
    "#     train_sampler = TrainSamplerMultiClassUnit(train_set, sample_unit_size=sample_unit_size)\n",
    "#     train_loader = DataLoader(train_set, batch_size=base_bs * ngpus, sampler=train_sampler, shuffle=False,\n",
    "#                               num_workers=4 * ngpus, pin_memory=True, drop_last=True)\n",
    "    #val_loader = DataLoader(val_set, batch_size=base_bs * ngpus, shuffle=False, num_workers=4 * ngpus,\n",
    "    #                        pin_memory=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=base_bs * ngpus, shuffle=False, num_workers=4 * ngpus,\n",
    "                             pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model.eval()\n",
    "    pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=not tqdm_on)\n",
    "#     output_list = torch.randn((6,10)).cuda()\n",
    "#     feature_list = torch.randn((6, 100)).cuda()\n",
    "    truth_list = []\n",
    "    pred_list = []\n",
    "    content_list = []\n",
    "#     m = torch.nn.AvgPool2d((1, 2000), stride=(1, 1950))\n",
    "    totalx = 0\n",
    "    totaly = 0\n",
    "    with torch.no_grad():\n",
    "        test_acc = AverageMeter()\n",
    "        for i, (x1, x2, x3, y, idx, content) in enumerate(pg):\n",
    "            x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
    "            pred = model(x)\n",
    "            correct = (pred.argmax(1) == y).sum().item()\n",
    "            totalx += correct\n",
    "#             print(correct)\n",
    "            totaly += len(y)\n",
    "            test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
    "            pg.set_postfix({\n",
    "                    'test acc': '{:.6f}'.format(test_acc.avg),\n",
    "                })\n",
    "            pred_label = np.argmax(pred.cpu().numpy(), axis=1).astype(np.int32)\n",
    "            idx = idx.cpu()\n",
    "            truth_label = y.cpu()\n",
    "            if i == 0:\n",
    "                pred_list = pred_label\n",
    "                truth_list = truth_label\n",
    "                idx_list = idx\n",
    "                content_list = content\n",
    "            else:\n",
    "                pred_list = np.append(pred_list, pred_label)\n",
    "                truth_list = np.append(truth_list, truth_label)\n",
    "                idx_list = np.append(idx_list, idx)\n",
    "                content_list = np.append(content_list, content)\n",
    "#                 print(pred_list)\n",
    "#                 print(truth_list)\n",
    "#             if i == 100:\n",
    "#                 break\n",
    "\n",
    "#     print(test_acc.avg)\n",
    "#     print(totalx)\n",
    "#     print(totaly)\n",
    "    correct_preds = [[] for i in range(num_authors)] # np.array([np.array([]) for i in range(10)])\n",
    "    wrong_preds = [[] for i in range(num_authors)] # np.array([np.array([]) for i in range(10)])\n",
    "\n",
    "    for i in range(len(idx_list)):\n",
    "        if truth_list[i] == pred_list[i]:\n",
    "            correct_preds[truth_list[i]].append(idx_list[i])\n",
    "        else:\n",
    "            wrong_preds[truth_list[i]].append([idx_list[i], pred_list[i]])\n",
    "        \n",
    "    header = ['index', 'author', 'is_correct', 'actual_pred', 'content']\n",
    "    table_name = f'./{table_name}'\n",
    "    f = open(table_name + \".csv\", 'w', encoding='UTF8')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for author in range(num_authors):\n",
    "        for correct_pred in correct_preds[author]:\n",
    "            writer.writerow([correct_pred, author, 1, author, content_list[correct_pred]])\n",
    "        for wrong_pred in wrong_preds[author]:\n",
    "            writer.writerow([wrong_pred[0], author, 0, wrong_pred[1], content_list[wrong_pred[0]]])\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing dataframe\n",
      "Number of authors:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-45d873934465>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nlp_test['Target'] = nlp_test['From']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "Training BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier of dim (196608 512 20)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './exp_data/21b15_val0.80590_e10.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-45d873934465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnlp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_author\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     test_bert(nlp_train,\n\u001b[0m\u001b[1;32m     23\u001b[0m               \u001b[0mnlp_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mtqdm_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c1270db5cc35>\u001b[0m in \u001b[0;36mtest_bert\u001b[0;34m(nlp_train, nlp_test, tqdm_on, num_authors, return_features, model_name, embed_len, ckpt_name, table_name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#     model.load_state_dict(torch.load(\"./exp_data/3b_val_bert-base-cased_coe1.0_temp0.1_unit2_epoch8/3b_val_val0.61900_finale7.pt\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./exp_data/{ckpt_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './exp_data/21b15_val0.80590_e10.pt'"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "source = \"turing\"\n",
    "df = load_dataset_dataframe(source)\n",
    "list_senders = [20]\n",
    "\n",
    "if source == \"imdb62\":\n",
    "        list_senders = [62]\n",
    "\n",
    "# start testing\n",
    "for limit in list_senders:\n",
    "    print(\"Number of authors: \", limit)\n",
    "    nlp_train, nlp_test = None, None\n",
    "    if source == \"turing\":\n",
    "        df = pd.read_csv('datasets/turing_AA_train_test.csv')\n",
    "        #print(df)\n",
    "        nlp_train = df[df['train'] == 1]\n",
    "        nlp_test = df[df['train'] == 0]\n",
    "        nlp_test['Target'] = nlp_test['From']\n",
    "    else:\n",
    "        nlp_train, _1, nlp_test, _2, _3 = build_train_test(df, source, limit, per_author=None, seed=1)\n",
    "    test_bert(nlp_train,\n",
    "              nlp_test,\n",
    "              tqdm_on=True,\n",
    "              return_features=True,\n",
    "             model_name='bert-base-cased',\n",
    "              num_authors=limit,\n",
    "              ckpt_name='21b15_val0.80590_e10.pt',\n",
    "              table_name='21b'\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
